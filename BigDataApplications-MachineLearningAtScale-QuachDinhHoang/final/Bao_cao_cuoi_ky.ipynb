{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BIG DATA APPLICATION:\n# MACHINE LEARNING AT SCALE\n"},{"metadata":{},"cell_type":"markdown","source":"ĐỀ TÀI:\n# TRÍCH XUẤT CHỦ ĐỀ TỪ CÁC BÀI BÁO \n"},{"metadata":{},"cell_type":"markdown","source":"    Sinh viên thực hiện\n    \n- VŨ ĐẶNG QUỲNH GIANG 17133015   \n\n- LÊ KHA\t             17133030\n\n- NGUYỄN THỊ NGỌC\t 17133041\n\n- LÊ MINH HÙNG\t     17133029\n"},{"metadata":{},"cell_type":"markdown","source":"# LỜI CAM KẾT\n\n Chúng em xin cam đoan đồ án “Trích xuất chủ đề từ các bài báo” là thành quả của quá trình học tập và tìm hiểu của nhóm dưới sự hướng dẫn của ThS. Quách Đình Hoàng. Không có bất kỳ sự sao chép hay sử dụng để bảo vệ một học vị nào. Tất cả những sự giúp đỡ cho việc xây dựng cơ sở lý luận cho bài luận đều được trích dẫn đầy đủ và ghi rõ nguồn gốc rõ ràng và được phép công bố. Các kết quả trình bày trong đồ án là hoàn toàn trung thực, nếu có sai sót và gian lận, chúng em xin chịu hoàn toàn trách nhiệm và mọi kỷ luật của giáo viên bộ môn và nhà trường đề ra."},{"metadata":{},"cell_type":"markdown","source":"# GIỚI THIỆU ĐỀ TÀI\n\n   Trong đề tài này, chúng tôi thực hiện việc thu thập các bài báo trên thế giới và tiến hành phân tích để khám phá những chủ đề mà các bài báo đang nói đến. Chúng tôi chọn Apache Spark làm framework xử lý dữ liệu chính của đề tài, do Spark hỗ trợ xử lý dữ liệu lớn với các thư viện machine learning được tích hợp sẵn.\n   \n   Trong đó việc phân tích có thể áp dụng các thuật toán PLSA và LDA. Việc ước lượng tham số trong mô hình PLSA sử dụng phương pháp maximum likelihood (thông qua thuật toán EM để tìm cực đại hàm likelihood). Việc ước lượng tham số trong mô hình LDA sử dụng phương pháp lấy mẫu Gibbs. \n   \n   Tại bước thu thập dữ liệu, chúng tôi sử dụng tập dữ liệu open-source trên trang arxiv.org, tập dữ liệu này có tên là NIPS(Neural Information Processing Systems) là một sự kiện hội thảo dẫn đầu thế giới trong lĩnh vực machine learning, bao gồm các chủ đề như deep learning, computer vision và reinforcement learning, ... Tập dataset này bao gồm tựa đề, tác giả, phần mở đầu, và full text của nội dung trong các bài viết của NIPS từ 1987 đến hiện tại. \n   \n   Sau đó chúng tôi sử dụng các thư viện trong Python, trong đó dùng thư viện pandas để tiền xử lý dữ liệu là chính (ngắt câu, ngắt từ, xóa ký tự lạ hoặc các stopword). Các thư viện để xây dựng mô hình như là os, findspark, pyspark, numpy, configuration. Trong đó dùng pyspark machine learning để xây dựng mô hình là chính.\n   \n   Thuật toán mà chúng tôi sử dụng là LDA, dùng để dự đoán tên cho từng chủ đề và biểu diễn mức độ đóng góp các chủ đề vào mỗi tài liệu. \n"},{"metadata":{},"cell_type":"markdown","source":"# MỤC TIÊU\n\n   Tiến hành xử lý dữ liệu dạng văn bản trên các bài báo, phân tích để tìm ra chủ đề chính của bài báo đang nói đến. Tìm ra hàng loạt các chủ đề được đề cập đến như công việc, thể thao, du lịch, khoa học, máy tính... rồi tập trung xóay sâu vào nó giúp chúng ta có thêm kiến thức, các kỹ năng mới, biết các xu hướng trong tương lai. "},{"metadata":{},"cell_type":"markdown","source":"**CHƯƠNG 1: TỔNG QUAN MÔ HÌNH CHỦ ĐỀ**\n\n1.\tXác định bài toán \n\n   Các mô hình chủ đề (giống như các mô hình xác xuất khác) giả định rằng tồn tại sẵn một mô hình với các tham số ẩn bên dưới dữ liệu (ở đây là các tài liệu trong bộ sưu tập tài liệu). Mỗi mô hình chủ đề cụ thể sẽ tìm cách ước lượng các tham số này sao cho nó khớp với dữ liệu nhất. Ý tưởng chính của các mô hình chủ đề cho dữ liệu văn bản là mỗi chủ đề là một phân bố xác xuất trên các từ (các chủ đề này được giả định đã tồn tại sẵn trong bộ sưu tập tài liệu), mỗi tài liệu là một phân bố xác xuất trên các chủ đề (nghĩa là, mỗi tài liệu đề cập đến một số chủ đề với các tỉ lệ nhất định), và mỗi từ được suy ra từ các chủ đề đó của tài liệu. Đầu vào và đầu ra mô tả như sau:\n   \n   Đầu vào: \n\tTập tài liệu C gồm n tài liệu: C = {d1, d2, d3, … , dn}.\n\tTập từ vựng V = {w1, w2, …, wm} chứa tất cả các từ riêng biệt có trong tập C\n\tSố chủ đề k mà ta cần phát hiện.\n   Đầu ra:\n\tTập chủ đề θ gồm k chủ đề: θ= {θ_1,θ_2,...,θ_n} mỗi chủ đề θ_i là một phân bố xác suất trên tập từ vựng V.\n  $\\sum_{w ∈ V} p(w|θ_i)=1$\n  \n\tMỗi tài liệu d_i có mức độ đóng góp của các chủ đề là $π_{i1},π_{i2},…,π_{ik}$\n"},{"metadata":{},"cell_type":"markdown","source":"*Các bước thực hiện: *\n\n- Bước 1: Tiến hành chọn tập dữ liệu mà chúng ta cần phân tích để xác định chủ đề \n\n- Bước 2: Tiền xử lý dữ liệu: bao gồm các công việc ngắt câu, ngắt từ sao cho có nghĩa, loại bỏ ký tự đặc biệt, các từ không đúng đắn, ... Chúng ta cũng loại bỏ các stopword như giới từ, từ nối vì chúng không thể hiện được chủ đề văn bản. Đưa các từ cùng gốc hoặc biến thể của chúng về từ gốc. \n\n- Bước 3: Áp dụng các mô hình chủ đề như PLSA, LDA… để trích xuất được các từ thể hiện được chủ đề của tập tài liệu. Xác định được k chủ đề và mức độ đóng góp của các chủ đề vào tài liệu từ đó có thể trực quan hóa để dễ theo dõi. Các mô hình áp dụng sẽ được trình bày kỹ ở phần sau. \n\n- Bước 4: Phân tích và đánh giá kết quả thu được.\n"},{"metadata":{},"cell_type":"markdown","source":"2.\tCác mô hình phân tích chủ đề\n\n    Mô hình PLSA\n    \n    Giới thiệu\n    \n    PLSA (Probabilistic Latent Semantic Analysis) có thể được xem là một trong những mô hình chủ đề tiên phong. PLSA xem tập tài liệu bao gồm nhiều văn bản, mỗi văn bản là một phân bố các chủ đề, mỗi chủ đề là một phân bố từ. Mô hình PLSA tìm cách ước lượng các tham số (các xác suất) sao cho khả năng sinh ra cả tập tài liệu là cao nhất. PLSA đã áp dụng thuật toán EM để xác định các tham số sao cho hàm log-likelihood đạt giá trị cực đại.\n    \n    Giải thích ý tưởng\n    \n    Với tập tài liệu C gồm n tài liệu, C = {d1, d2, · · · , dn}. Mỗi tài liệu di bao gồm nhiều từ khác nhau và tập tài liệu V là các từ có trong tập tài liệu C. Ta xác định tần số xuất hiện của các từ w_j có trong tài liệu d_i gọi là c (w_j, d_i). Với các chủ đề ẩn θ ta xem mỗi θ_i là một phân bố xác suất trên tập từ vựng V, tức là θ_i bao gồm các từ trên tập từ vựng V với xác suất mỗi từ là khác nhau, xác suất càng cao thì tức là chủ đề đó có liên quan đến từ w_j. Ta cũng xem mỗi tài liệu d_i là một phân bố xác suất trên tập chủ đề θ, tức là mỗi tài liệu di đều bao gồm tất cả các chủ đề với xác suất mỗi chủ đề là khác nhau và được thể hiện trong bảng, xác suất p(θ_j|d_i) càng cao thì khả năng cao θ_i là chủ đề được đề cập nhiều trong tài liệu $d_i$.\n    \n    Công thức mô tả PLSA: \n    \n    $p(w|d) = \\sum_{j=1}^k p(θ_j )p(w|θ_j )=\\sum_{j=1}^k π_{di}p(w|θ_j )$\n    \nTrong đó\n\n- $p(w,d)$ là xác suất từ w thuộc tài liệu d\n\n- $p(θ_j)$ là độ bao phủ (tỷ lệ) của chủ đề $θ_j$ trong tài liệu d\n\n- $p(w|θ_j)$ là xác suất từ w thuộc chủ đề $θ_j$.\n\n    Xác suất sinh ra tài liệu d được tính từ p(w,d) như sau:\n    \n    $p(d) = \\Pi_{w∈d}[\\sum_{j=1}^{k} p_(θ_j)p(w|θ_j )]$\n\n   Đặt:\n\n- c(w,d) là tần số xuất hiện của từ w trong tài liệu d\n- V(d) là tập từ vựng của tài liệu d (các từ khác nhau trong tài liệu d)\n\tKhi đó p(d) được viết lại thành\n    \n    $p(d) = \\Pi_{w∈V(d)}[\\sum_{j=1}^{k} p_(θ_j)p(w|θ_j )]^{c(w,d)}$\n\n\tĐể dễ tính toán và giảm sai số, người ta thường lấy log của xác xuất này. log p(d) chính là hàm likelihood.\n\n    $log_{p(d)} = \\sum_{wϵV(d)}[c(w,d)log[\\sum_{j=1}^{k} p_(θ_j)p(w|θ_j )]]$\n\n    Nhận xét về mô hình PLSA\n    \n    Mặc dù mô hình PLSA sử dụng mô hình xác suất để biểu diễn tập tài liệu và ước lượng các giá trị tham số một cách tối ưu. Tuy nhiên mô hình PLSA còn tồn tại các khuyết điểm:\n- Khi số lượng tài liệu trong tập tài liệu tăng dẫn đến các tham số xác suất tăng tuyến tính gây overfitting.\n\n- Không thể xác định được các tham số xác suất cho những tài liệu mới, nằm ngoài tập huấn luyện.\n\n    Một thuật toán cải tiến của PLSA là LDA sẽ giúp khắc phục các hạn chế còn tồn đọng của PLSA.\n\n    Mô hình LDA\n\n    Ý tưởng mô hình LDA\n    \n    Dựa trên ý tưởng của PLSA, mô hình LDA (Latent Dirchlet Allocation) cũng tìm cách ước lượng xấp xỉ tham số các xác suất dùng Bayesian inference thay vì dùng maximum likelihood. Phương pháp lấy mẫu Gibss (Gibss sampling) phương pháp phổ biến để xác định các tham số đó.\n    \n    Để hiểu rõ hơn về mô hình này chúng ta đi vào tìm hiểu ví dụ trong Hình 2. Trong tiến trình này, các chủ đề được mô hình giả định là một phân phối các từ được sinh ra trước đó. Với ví dụ trong Hình 2 thì chủ đề 1 sẽ là tập hợp các từ {“gene”, “dna”, “genetic”} với các xác suất tương ứng là {0.04, 0.02, 0.01}. Ngoài ra, có một điểm cần phải lưu ý là các phân phối xác suất của các từ trong chủ đề hay phân phối các chủ đề trong tài liệu của mô hình LDA đều tuân theo phân phối Dirichlet.\n    \n    Ta thực hiện miêu tả tiến trình sinh mẫu trong ví dụ trên như sau: đầu tiên ta giả sử có một lượng chủ đề tồn tại trong tập tài liệu, mỗi chủ đề là một phân phối các từ, sau đó ta thực hiện lựa chọn một phân phối các chủ đề cho tài liệu (biểu đồ histogram bên phải). Sau đó đối với từng từ trong chủ đề trong biểu đồ, ta chọn một phép gán chủ đề (là các hình tròn), sau đó là lựa chọn từ trong chủ đề đó để đưa vào tài liệu của ta.\n\n    Quá trình sinh LDA\n    \n    Có một số kí hiệu ta cần làm rõ như sau\n    \n    Kí hiệu\tMô tả kí hiệu\n    \n    α\tTham số đầu vào cho phân phối Dirichlet của tài liệu\n    \n    β\tTham số đầu vào cho phân phối Dirichlet của chủ đề\n    \n    θ\tPhân bố chủ đề ứng với mỗi tài liệu\n    \n    φ\tPhân bố các từ ứng với mỗi chủ đề\n    \n    z\tChỉ số chủ đề được lựa chọn\n    \n    w\tTừ trong chủ đề\n\n    K\tLà tập các chủ đề\n    \n    N\tLà tập các tài liệu\n    \n    N_D\tLà tập các từ trong tài liệu\n\n    Tiến trình sinh mẫu LDA được thể hiện theo quy trình sau\n   \n    Với mỗi tài liệu d∈{1,…,D}, chọn một phân bố chủ đề từ tài liệu θ_d theo phân bố Dirichlet, θ_d ~ Dirichlet(α)\n\n\tVới mỗi chủ đề k∈{1,…,K}, chọn một phân bố từ từ chủ đề φ_k theo phân bố Dirichlet, φ_k~Dirichlet(β)\n    \n\tVới mỗi từ t∈{1,…,T) trong tập d\n\n- Chọn phép gán chủ đề Z_(d.t) từ θ_d,Z_(d.t)  ~ Mutinomial(θ_d)\n\t\n- Chọn từ W_(d,t) từ φ_k,W_(d,t)~Mutinomial(φ_k)\n\n    $$P(W,Z,θ,φ,α,β) = \\Pi_{d=1}^D P(θ_d;α) \\Pi_{k=1}^KP(φ_k;β) \\Pi_{t=1}^TP(Z_{d,t}|θ_d)P(W_{d,t}|φ_K Z_{d,t})$$\n    \n     Nhận xét về mô hình LDA\n     \n     Mô hình LDA có một số điểm nổi bật so với mô hình PLSA:\n     \n- Số lượng tham số ít hơn PLSA và không tăng tuyến tính khi số lượng tài liệu trong tập tài liệu tăng lên.\n\n- LDA thể hiện đầy đủ được tầm quan trọng của chủ đề trong tập tài liệu cũng như tầm quan trọng của từ trong chủ đề thông qua độ sai lệnh lớn giữa các giá trị xác suất.\n\n- Quá trình sinh văn bản phù hợp hơn và kết quả sinh ra văn bản đã tốt hơn. \n"},{"metadata":{},"cell_type":"markdown","source":"3.\tLatent Dirichlet allocation (LDA)\n\n    Latent Dirichlet allocation (LDA) là một mô hình chủ đề có khả năng đưa ra các chủ đề từ tập hợp các tài liệu văn bản.\n    \n    Dữ liệu đầu vào (featuresCol): LDA được cung cấp một tập hợp (collection) các tài liệu (document) làm dữ liệu đầu vào, thông qua tham số featuresCol. Mỗi tài liệu được chỉ định dưới dạng Vectơ có độ dài vocabSize, trong đó mỗi mục là số lần xuất hiện của thuật ngữ (từ) tương ứng trong tài liệu. Các lớp biến đổi feature như pyspark.ml.feature. Tokenizer và pyspark.ml.feature.CountVectorizer là những công cụ phổ biến để chuyển đổi văn bản thành vectơ đếm từ (word count vector).\n    \n    Thuật toán LDA trong MLlib có các tham số sau:\n\n    k:\tSố lượng chủ đề (tức là các trung tâm cụm) \n    \n    optimizer:\tTrình tối ưu hóa để sử dụng để học mô hình LDA. \n    Spark cung cấp hai thuật toán tối ưu hóa, bao gồm EM algorithm và Online optimizer.\n    \n    docConcentration:\tTham số Dirichlet cho các bản phân phối trước trên tài liệu theo chủ đề. \n    Còn được biết tới như tham số alpha.\n        \n    topicConcentration:\tTham số Dirichlet cho các phân phối trước của chủ đề qua các thuật ngữ (từ). \n    Còn được biết tới như tham số beta.\n    \n    maxIterations:\tGiới hạn số lần lặp lại.\n    \n    checkpointInterval:\tNếu sử dụng checkpointing (cài đặt trong cấu hình Spark), tham số này chỉ định tần suất mà các điểm dừng checkpoint sẽ được tạo. \n    Nếu maxIterations có giá trị lớn, sử dụng checkpoint có thể giúp giảm kích thước dữ liệu bị xáo trộn trên đĩa và giúp khắc phục lỗi.\n\n"},{"metadata":{},"cell_type":"markdown","source":"4.\tTiền xử lý dữ liệu\n\n    Quá trình xây dựng một mô hình chủ đề nói riêng và mô hình học máy nói chung đều bao gồm việc nhận vào một số nội dung văn bản được xác định trước và thực hiện trên đó một số phân tích và biến đổi cơ bản, để đầu ra phục vụ cho thành phần chính của quá trình xây dựng mô hình xử lý ngôn ngữ tự nhiên (NLP). \n    \n    Tiền xử lý dữ liệu gồm có các bước:\n\n> Tokenize dữ liệu văn bản\n\nMột trong những bước đầu tiên cho NLP là tokenize văn bản. \n \nTokenization là việc chia nhỏ các chuỗi văn bản thành các phần nhỏ hơn, hay còn gọi là “token”. Các đoạn văn có thể được mã hóa (tokenize) thành các câu văn, sau đó câu văn có thể được mã hóa thành các từ.\n\nTokenization còn được biết tới như phân đoạn văn bản (text segmentation) hoặc phân tích từ vựng (lexical analysis). Đôi khi bước phân đoạn được thực hiện để chia nhỏ một lượng văn bản lớn thành các cụm nhỏ hơn, nhưng lớn hơn một từ đơn, ví dụ như một đoạn văn hoặc câu văn. Ngược lại, mã hóa (tokenization) được dành riêng cho quá trình phân tích văn bản thành từng từ.\n\nQuá trình tokenize văn bản có nhiều mức độ từ cơ bản đến phức tạp, tùy thuộc vào bản chất của văn bản đầu vào. Làm thế nào để ta có thể xác định được các câu trong một văn bản? Đôi lúc dấu chấm câu ở đầu câu không phải là dấu chỉ ngắt đoạn. \n\nVí dụ một câu đơn giản mà dấu chấm câu là dấu ngắt đoạn:\n\n-\tThe quick brown fox jumps over the lazy dog.\n\nVí dụ một câu mà trong đó dấu chấm câu không phải là dấu ngắt đoạn:\n\n-\tDr. Ford did not ask Col. Mustard the name of Mr. Smith's dog.\n\nSau khi một đoạn văn bản đã được mã hóa (tokenize) thì các bước xử lý tiếp theo sẽ được thực hiện.\n\n> Chuẩn hóa dữ liệu văn bản (Normalization)\n\nBước chuẩn hóa nhằm mục đích chuyển đổi văn bản thành một kiểu dữ liệu đồng nhất. Chuẩn hóa văn bản có thể gồm nhiều công việc, nhưng chúng tôi sẽ tiếp cận chuẩn hóa theo 3 bước khác nhau: (1) stemming, (2) lemmatization và (3)\ncác bước khác.\n\n- Stemming\n\n    Cắt từ (stemming) là quá trình loại bỏ các phụ tố (affix) khỏi một từ để có được từ gốc. Một phụ tố bao gồm các dạng như: \n    \n•\tTiền tố (prefix)\nVí dụ: sub-mit, pre-determine, un-willing\n•\tHậu tố (suffix) \nVí dụ: wonder-ful, depend-ent, act-ion\n•\tTiếp tố (infix)\nVí dụ: cups-ful, spoons-ful, passers-by\n\n- Lemmatization\n\n    Bổ đề (lemmatization) liên quan đến công việc stemming, bổ đề khác ở điểm rằng nó có thể nắm bắt các dạng chính tắc của một từ dựa trên gốc của từ đố.\n    \n    Ví dụ: Nếu ta cho stem từ \"better\" sẽ không trả về dạng gốc của nó; tuy nhiên, lemmatization sẽ giúp đạt được kết quả sau: “better” thành “good”\n\n- Các bước khác\n\n    Stemming và lemmatization là những phần chính của việc tiền xử lý văn bản, là bước quyết định mô hình của chúng tôi có cho ra kết quả chính xác nhất hay không. Stemming và lemmatization đòi hỏi sự hiểu biết sâu sắc về các quy tắc và chuẩn mực về ngữ pháp.\n    \n    Tuy nhiên, có nhiều bước khác cần được thực hiện để biến đổi sao cho toàn tập văn bản đồng bộ với nhau, trong đó nhiều bước chỉ gồm những việc đơn giản như thay thế từ hoặc loại bỏ từ. Tuy nhiên, chúng không kém phần quan trọng đối với quá trình tổng thể. Các bước bao gồm:\n    \ni.\tChuyển các từ có chữ cái hoa thành chữ cái thường.\n\nii.\tLoại bỏ số (hoặc chuyển đổi số thành biểu diễn văn bản).\n\niii.\tLoại bỏ dấu chấm câu (thường là một phần của bước tokenization, nhưng vẫn đáng lưu ý ở bước này).\n\niv.\tLoại bỏ khoảng trắng đầu câu và cuối câu (thường cũng là một phần của mã hóa).\n\n> Loại bỏ stop word\n\nStop words là những từ được lọc ra trước khi tiếp tục xử lý văn bản, vì những từ này ít đóng góp vào ý nghĩa tổng thể, vì chúng thường là những từ phổ biến nhất trong một ngôn ngữ. \n    \nVí dụ: \"the\", \"and\" và \"a\", những từ trong một đoạn văn cụ thể, nói chung không đóng góp nhiều vào nội dung. \n    \nVí dụ đơn giản về việc loại bỏ stop word:\n    \n (The) quick brown fox jumps over (the) lazy dog.\n    \nCâu trên vẫn rất dễ đọc và dễ hiểu sau khi các stop word đã bị xóa.\n\n> Vector hóa dữ liệu\n\nTần suất tài liệu nghịch đảo tần số thuật ngữ (TF-IDF): Term frequency-inverse document frequency là một phương pháp vector hóa feature được sử dụng rộng rãi trong text mining để phản ánh mức độ quan trọng của một thuật ngữ đối với một tài liệu trong kho ngữ liệu.\n    \nVì vậy, nếu bước tiền xử lý văn bản được thực hiện một cách kỹ lưỡng có thể giúp tăng độ chính xác của các công việc xử lý văn bản tự nhiên.\n"},{"metadata":{},"cell_type":"markdown","source":"**CHƯƠNG 2: ỨNG DỤNG TOPIC MODEL PHÂN TÍCH DỮ LIỆU LỚN**"},{"metadata":{},"cell_type":"markdown","source":"1.\tDữ liệu và tiền xử lý dữ liệu\n\n- Mô tả tập dữ liệu nghiên cứu\n\n    Neural Information Processing Systems (NIPS) là một trong những hội nghị về máy học hàng đầu trên thế giới. Nó bao gồm nhiều chủ đề, từ deep learning và thị giác máy tính đến khoa học nhận thức (cognitive science) và học tăng cường (deep learning).\n    \n    Bộ dữ liệu NIPS này bao gồm tiêu đề, tác giả, tóm tắt và văn bản trích xuất cho tất cả các bài báo của NIPS từ hội nghị đầu tiên năm 1987 đến hội nghị năm 2016 hiện tại. Dataset được trích xuất từ các tệp PDF thô và được sang lưu dưới dạng CSV và dạng cơ sở dữ liệu SQLite. \n\n    Số tập dữ liệu và chi tiết các file\n    \n    Tên |\tMô tả file |\tSố dòng\tCác cột \n    \nauthors.csv |\tTên các tác giả và số id của mỗi người |\t9719\t\n    \n•\tid: numeric\n\n•\tname: string\n\npapers.csv |\tNăm, tiêu đề, thể loại của bài viết, tóm tắt và văn bản trích xuất từ tất cả các bài báo của hội nghị NIPS |\t7284\t\n    \n•\tid: numeric\n\n•\tyear: numeric\n\n•\ttitle: string\n\n•\tevent_type: string\n\n•\tpdf_name: string\n\n•\tabstract: string\n\n•\tpaper_text: string\n\n\npaper_authors.csv |\tFile map giữa các bài nghiên cứu và tác giả | \t20800\t\n\n•\tid: numeric\n\n•\tpaper_id: numeric\n\n•\tauthor_id: numeric\n\n- authors.csv\n\nTên cột|\tMô tả cột|\tDữ liệu mẫu\n\nid|\tSố thứ tự của tác giả trong tập dữ liệu|\t1\n\nname|\tTên tác giả đã tham gia| \tHisashi Suzuki\n\n- papers.csv\n\nTên cột |\tMô tả cột |\tDữ liệu mẫu\n\nid |\tSố thứ tự của bài viết trong tập dữ liệu |\t1\n\nyear |\tNăm phát hành của bài viết |\t1987\n\ntitle |\tTựa đề của bài viết\tSelf-Organization of Associative Database and Its Applications\n\nevent_type\tThể loại của bài thuyết trình cho bài viết này |\tOral\n\npdf_name |\tTên của file pdf |\t1-self-organization-of-associative-database-and-its-applications.pdf\n\nabstract |\tĐoạn văn bản của abstract của bài viết |\tAbstract Missing\n\npaper_text |\tToàn bộ văn bản của bài viết đã được trích xuất ra |\t767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nAND ITS APPLICATIONS\\nHisashi Suzuki and Suguru Arimoto […]\n\n- paper_authors.csv\n\nTên cột |\tMô tả cột |\tDữ liệu mẫu\n\nid |\tSố thứ tự của dòng trong tập dữ liệu |\t1\n\npaper_id |\tId của bài viết |\t63\n\nauthor_id |\tId của một tác giả tham gia viết bài đó |\t94\n\nLưu ý: Một bài viết có thể được cống hiến bởi nhiều tác giả.\n\nDo quan sát chúng tôi thấy tập dữ liệu paper.csv chứa đầy đủ nhất những đặc tính chúng tôi cần để làm đầu vào của mô hình chủ đề, nên chúng tôi sẽ tập trung phân tích và xử lý trên file này.\n\n- Tiền xử lý dữ liệu\n\n    Dựa theo đó, chúng tôi tiến hành tiền xử lí dữ liệu trong file papers.csv.\n    \n•\tLoại bỏ các cụm ký tự đặc biệt.\n\nVí dụ: ký tự xuống hàng, ký tự format của UTF-8, ...\n\n•\tChuyển các từ có chữ cái hoa thành chữ cái thường.\n\n•\tLoại bỏ các dấu chấm câu.\n\n•\tLoại bỏ cái từ chỉ có một chữ cái.\n\nVí dụ: “a”, “b”, “c”\n\n•\tLoại bỏ các ký tự trống đầu chuỗi và cuối chuỗi.\n\n•\tThay thế hai ký tự khoảng trắng kế nhau thành một ký tự khoảng trắng.\n\n•\tLoại bỏ các stopwords bằng dữ liệu tải về từ thư viện nltk theo ngôn ngữ tiếng Anh.\n\n•\tBổ ngữ (lemmatization) với tập dữ liệu, chuyển từ thành loại từ gốc của nó.\n\n•\tVector hóa dữ liệu: ở đây chúng tôi chọn cột feature là cột paper_text trong tập dữ liệu papers.csv để chuyển đổi thành vector.\n"},{"metadata":{},"cell_type":"markdown","source":"2.\tXây dựng Mô hình LDA \n\n    Train mô hình LDA\n    \n    Huấn luyện mô hình LDA với các tham số:\n\n\tk: Số chủ đề = 10\n\n\tmaxIter: Số lần lặp trong môi trường kiểm nghiệm = 50\n\n\tfeaturesCol: cột dữ liệu feature = cột dữ liệu đã được vector hóa\n\n\toptimizer: trình tối ưu thuật toán = thuật toán Expectation-Maximization \n"},{"metadata":{},"cell_type":"markdown","source":"3.\tBiểu diễn kết quả\n\n    WordClouds cho từng chủ đề\n    \n    Với tham số k = 10, các chủ đề kết quả không thể hiện rõ mức độ phân cụm. Chúng tôi thay đổi lại cho k = 4 và chạy lại model thì thể hiện được mức độ phân cụm rõ ràng hơn.\n\n"},{"metadata":{},"cell_type":"markdown","source":"4. Giải thích kết quả\n\n    Chúng tôi xin nhận xét rằng các chủ đề được sinh ra có phân bố từ đều có liên quan mật thiết đến các khái niệm thường gặp trong lĩnh vực học máy.\n    \n    Do bản chất đó của tập dữ liệu, nên chúng tôi đã nâng cao số lượng từ phổ biến trong toàn bộ tập dữ liệu lên thành 500 từ. \n    \n    Nếu số từ phổ biến để loại đi dưới 500, chúng tôi thấy xuất hiện hiện tượng các từ khóa đó xuất hiện đồng đều trong phân bố của các chủ đề kết quả, gây ra sự phân cụm kém.\n    \n    Nếu số từ phổ biến để loại đi cao hơn 500, chúng tôi thấy xuất hiện hiện tượng các từ khóa quan trọng định hình topic bị lược bỏ đi mất, chỉ còn lại những từ khóa định tính kém cho chủ đề, gây mất đi những thông tin thiết yếu.\n    \n    Với mô hình LDA đã khởi tạo, chúng tôi thấy được kết quả phân cụm như sau:\n    \n-Chủ đề 1: input, train, image, neural, network\n\n-Chủ đề 2: network, synapse, weight, figure\n\n-Chủ đề 3: use, case, train, test, system\n\n-Chủ đề 4: example, function, test, approach, use\n\n   Giải nghĩa kết quả phân cụm chủ đề:\n    \n-Chủ đề 1: nói về computer vision sử dụng neural network\n\n-Chủ đề 2: nói về việc tinh chỉnh tham số trong machine learning\n\n-Chủ đề 3: nói về việc chạy thử và test một hệ thống\n\n-Chủ đề 4: nói về việc sử dụng một phương thức để chạy model\n"},{"metadata":{},"cell_type":"markdown","source":"**CHƯƠNG 3: KẾT LUẬN**"},{"metadata":{},"cell_type":"markdown","source":"3.\tKết quả đạt được\n\n    Bằng mô hình chủ đề này chúng tôi có thể nhanh chóng áp dụng bất kỳ tập dữ liệu nào để tìm ra được các từ khóa chính thể hiện nội dung của văn bản đó. Chúng tôi nắm bắt được các chủ đề nổi trội được nhiều người nghiên cứu và quan tâm, chúng tôi biết được khuynh hướng phát triển của lĩnh vực khoa học dữ liệu, qua đó để tập trung khai thác những vấn đề cũng như đón đầu xu thế trong cộng đồng nghiên cứu.\n    \n    Thông qua việc thực hiện đề tài, chúng tôi biết được phân tích theo chủ đề đang trở thành một trong những hướng nghiên cứu rất phát triển, đặc biệt là tại các doanh nghiệp. Chẳng hạn như các mạng xã hội Facebook, Youtube, các trang bán hàng điện tử Amazon, ... đang thu thập và phân tích các bình luận, các nội dung do người dùng đăng tải để phát hiện mối quan tâm nhằm đưa ra các giải pháp kinh doanh hiệu quả.\n"},{"metadata":{},"cell_type":"markdown","source":"3.\tHướng phát triển\n\n    Dù kết quả xây dựng mô hình có giới hạn, nhưng chúng tôi cũng tự nhận thấy đây là một đề tài đầy triển vọng. Sẽ phát triển đề tài nhưu sau:\n    \ni.\tTinh chỉnh các tham số, để cho ra kết quả mô hình chính xác nhất.\n\nii.\tÁp dụng phân tích chủ đề theo dữ liệu của từng năm, để thấy được sự thay đổi theo từng năm của các topic nổi trội trong lĩnh vực khoa học dữ liệu.\n\niii.\tÁp dụng phân tích chủ đề theo từng tác giả.\n\niv.\tXây dựng trang web thể hiện và trực quan hóa kết quả của mô hình học máy.\n"},{"metadata":{},"cell_type":"markdown","source":"**Tài liệu tham khảo**"},{"metadata":{},"cell_type":"markdown","source":"-\tCheng Xiang Zhai and Sean Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining. ACM Books, 2016.\n\n-\tDavid Mimno (2018), Topic Models Material adapted\n\n-\tNguyễn Quốc Đạt, Trần Thị Tố Uyên, Cao Xuân Nhẫn (2019), Tìm hiểu web scraping và topic analysis để phân tích các thông báo tuyển dụng. Khóa luận tốt nghiệp kỹ sư công nghệ thông tin, trường Đại học Sư phạm Kỹ thuật TP. HCM\n\n-\tPhạm Đình Khánh (2019), Bài 10 Thuật toán LDA - Xác định Topic, từ https://phamdinhkhanh.github.io/2019/09/08/LDATopicModel.html\n\n-\tDamir Valput (2020), Topic modelling: interpretability and applications, từ https://datascience.aero/topic-modelling/\n\n-\tMatthew Mayo (2017), A General Approach to Preprocessing Text Data, từ https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html\n\n-\tTài liệu tham khảo mã nguồn của thư viện ML LDA trong Spark, ở https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/clustering.html#LDA\n"}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}