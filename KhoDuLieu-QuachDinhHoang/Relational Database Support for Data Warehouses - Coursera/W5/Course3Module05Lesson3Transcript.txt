Welcome to Lesson 3 of Module
5 on Physical Design and Governance of Data Warehouses. Let's have some fun
learning about big data. I'm gonna start with
an important conceptual question that I want you to think
about throughout this lesson. What dimension of big data
is most important for physical design of data warehouses? Lesson three extends conceptual
background about storage and processing architectures from lessons one
and two to issues involving big data. Lesson three covers big data definitions,
concepts, and opportunities. Big data challenges and opportunities
impact data warehouse storage and processing architecture decisions. You have four learning
objectives in this lesson. You should provide definitions of big
data and discuss dimensions of big data. To speak precisely about big data, you should define units of big data with
associated examples to depict each unit. As a more reflective call,
you should think about value areas for big data and the impact of these
areas on data warehouse requirements. The phenomenon of explosive data growth,
known as big data, was first articulated by Doug Laney
of the META Group in 2001. According to Laney's report, data management challenges involve three
dimensions, volume, the quantity of data, velocity, the rate of generating and
processing data, and variety. For variety, Laney emphasized
data integration issues of incompatible formats, inconsistent data
semantics in differing data structures. Now variety means type of data, especially
the distinction between structured and unstructured data,
complementing the dimensions of big data. And McKinsey Global Institute definition
provides flexibility to varied volumes considered as big data by technology,
industry sector, and time. Another definition of big data by John
Akred emphasizes informed decision making about large diverse datasets facilitated
by various information technologies. Big data has been an important issue for
almost 20 years, with considerable growth in the emphasis on big data
during the latter part of this period. The first documented usage of the term
big data occurred in a 1997 paper about data visualization written by
computer scientists at USA NASA agency. In the first attempt to be
precise about big data, Professor Hal Varian of UC-Berkeley
wrote influential papers in 2000 and 2003 with estimates of the total
amount of digital data in the world. Doug Laney's report in 2001 was noted
by some business executives and government officials. Work on the map-produced
algorithm at Google and the Hadoop open-source project provided
an important enabling technology. A 2008 report written by
Professor Randy Katz and others popularized the term and
lead to increased government funding for research and
commercial ventral capitol funding. The development of Hadoop 2 in 2013
provided a more powerful enabling technology hat is becoming widely used and
incorporated into integration tools. The original big data dimensions
are volume velocity and variety have been augmented with
other v terms in recent years, this info graphic from IBM provides
a concise visual representation of big data dimensions along with examples
to clarify each dimension. I encourage you to review
this graphic carefully at the source provided
in the lecture notes. The fourth v term in the graphic, veracity, is somewhat controversial
as a dimension of big data. According to IBM, veracity's uncertainty in data
increasing as other dimensions increase. However, some have argued that veracity
is more a function of measurement devices rather than data size. For example, with satellite images,
sensors generate data along with algorithms and human analysts,
to detect hidden features, such as street numbers painted on curbs
and business names in satellite images. The growth in data comes
from a variety of sources, including sensors in smart phones,
energy meters, wearables and automobiles. Interaction of individuals in social media
websites, high resolution images generated by satellites, and digitized multimedia
content, especially high definition video. Here are some concrete
sources of big data. 1 billion websites with
7 trillion web pages, 30 billion content pieces
on Facebook each month. By 2020, 152 million connected cars
with hundreds of sensors per car. Climate analysis using thousands
of sensors in space and on the ground recording measurements about
weather, land use, vegetation, oceans, ice cover, precipitation, drought,
water quality and many more variables. 6 billion hours of video
watched each month on YouTube with a hundred hours
uploaded every minute. To effectively manage big data, you should have a clear understanding
of big data volume units. Basic units of data are the byte,
one character eight bits, a kilobyte known as KB,
megabyte, MB and gigabyte, GB. A kilobyte is either 1024 bytes,
that is a binary system measure, or 1000 bytes, a metric system measure. Since computers are binary machines,
Kilo means 1,024 for digital storage. Storage manufacturers often
use the metric system measure, perhaps because Americans don't
understand binary measures. However using the metric system
measure provides less capacity than the binary system measure. Thus a Megabyte denotes either 1,000 or
1,024 Kilobytes. While a gigabyte denotes 1,000,
1,024 megabytes. To clarify your understanding
of big data units, you should grasp examples
of large data units. This table extends the basic
units to cover larger units along with big data examples. Terabyte was big data in previous decades. Now it is the typical capacity of
hard drives and personal computers. Petabyte is yesterday's big data, as shown by past data requirements at
Google to process petabyte levels. Exabyte is generally considered
as the current big data level, as demonstrated by Internet traffic rates. Internet traffic is rapidly approaching
the Zettabyte level in 2016. Increasing from the Exabyte range in 2013. An estimate of the U.S.A National Security
data center capacity is 1 Yottabyte, although this estimate is speculative
as the operations of the data center are highly classified. Trends in data volumes indicate
the growing importance of big data in organizations in society. According to a 2013 study by a large
Scandinavian research organization, 90% of the world's data in 2013
was generated just since 2011. From 2013 to 2020,
digitized data will grow 10 times from 4.4 trillion gigabytes to 44 trillion
gigabytes, with doubling every two years. According to a study by IDC, only 22% of
digital data was useful for analysis. By 2020 the useful percentage could
grow to more than 35% mostly because of the growth of data from sensors. IBC estimates that only 5% of digital
data was especially valuable in 2013. By 2020, the percentage of especially
valuable data could double. However, in 2013 the available
storage capacity could hold just 33% of the digitized data. By 2020,
it'll be able to store less than 15%. Big data creates opportunities
if managed well. Here are some important high
value areas for big data. Promotions to target consumers, based on
individual or small group preferences. Risk data management in auto insurance to reduce information asymmetry by using
sensors to monitor driving behavior. Inventory management in supply chains
using radio frequency identification tags to track product location and
condition. Surveillance about terrorist threats for
nations and theft reduction for businesses. Military applications for
enemy tracking and battle management. Entertainment with new
digital products and monitoring of consumer
usage of digital products. Lesson three. Change directions from details
about physical design technologies to increasing demands for
handling large, diverse data sets. You learn definitions of big data,
dimensions of big data, data unit sizes, and sources of big data. Organizations worldwide grapple with
challenges and opportunities of big data. In answer to the opening question, volume
is the major concern for data warehouses. Challenges in data integration processing mainly involve refreshing large volumes
of data in limited time windows. With international operations, time windows can be short to
complete daily refresh processing. Challenges in business
intelligence reporting involve efficient retrieval of large
volumes of data to summarize and analyze. Veracity is also a major
challenge in data integration with different levels of data quality and
standards in storage systems. Poor data quality can require
labor-intensive investigations to resolve. Since data warehouses
are repositories of secondary data, velocity is not usually a major issue. For operational databases,
velocity is the major issue. Besides volume and porosity issues,
you should understand that big data provides tangible benefits to
justify initial development and major extensions of data warehouses. For example, read to organizations see
opportunity for dynamic pricing based on inventory demand in approved product
recommendations on websites. Package delivery firms see opportunity
to use sensors to track goods and delivery vehicles in optimized
routes through live traffic data. These business opportunities drive
demands on data warehouses, for data integration and
business intelligence reporting.